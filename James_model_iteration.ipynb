{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bumho/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.tsv\", sep= '\\t')\n",
    "test = pd.read_csv(\"data/test.tsv\", sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_phrase(phrase):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", phrase)\n",
    "    lower_case = letters_only.lower()\n",
    "    \n",
    "    words = lower_case.split()\n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(\" \".join(words))    \n",
    "\n",
    "def num_words(phrase):\n",
    "    return len(phrase.split())\n",
    "\n",
    "def length_phrase(phrase):\n",
    "    return len(phrase)\n",
    "\n",
    "def avg_word_length(phrase):\n",
    "    if(phrase != ''):\n",
    "        return sum(map(len, phrase.split()))/len(phrase.split())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "most_positive = ['remarkable', 'brilliant', 'terrific', 'excellent', 'finest', 'extraordinary', 'masterful', \n",
    "                 'hilarious', 'beautiful', 'wonderful', 'breathtaking', 'powerful', 'wonderfully', 'delightful', \n",
    "                 'masterfully', 'fantastic', 'dazzling', 'funniest', 'interference', 'refreshing']\n",
    "most_negative = ['worst', 'failure', 'lacks', 'waste', 'bore', 'depressing', 'lacking', 'stupid', 'disappointment', \n",
    "                 'unfunny', 'lame', 'devoid', 'trash', 'lousy', 'junk', 'poorly', 'mess', 'sleep', 'unappealing', 'fails']\n",
    "\n",
    "def contains_positive(phrase):\n",
    "    for word in phrase.split():\n",
    "        if word in most_positive:\n",
    "            return 1 \n",
    "    return 0\n",
    "        \n",
    "def contains_negative(phrase):\n",
    "    for word in phrase.split():\n",
    "        if word in most_negative:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getSentences(data):\n",
    "    sentenceK = data['SentenceId'].drop_duplicates()\n",
    "    sentences = data.iloc[sentenceK.keys()]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_transform(data):\n",
    "    data['CleanPhrase'] = data['Phrase'].apply(clean_phrase)\n",
    "    data['NumWords'] = data['CleanPhrase'].apply(num_words)\n",
    "    data['LengthPhrase'] = data['CleanPhrase'].apply(length_phrase)\n",
    "    data['AvgWordLength'] = data['CleanPhrase'].apply(avg_word_length)\n",
    "    data['ContainPositive'] = data['CleanPhrase'].apply(contains_positive)\n",
    "    data['ContainNegative'] = data['CleanPhrase'].apply(contains_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apply_transform(train)\n",
    "apply_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = getSentences(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51919124553222373"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = [\"ContainPositive\", \"ContainNegative\", \"NumWords\", \"LengthPhrase\", \"AvgWordLength\"]\n",
    "# predictors = [\"ContainPositive\", \"ContainNegative\"]\n",
    "# alg = LogisticRegression(random_state=1)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=1000, min_samples_split=8, min_samples_leaf=4)\n",
    "cross_validation.cross_val_score(alg, train[predictors], train[\"Sentiment\"], cv=3).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to come up with better predictors because they would only give around 50 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', OneVsOneClassifier(LinearSVC())),\n",
    "                     ])\n",
    "# pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', MultinomialNB()),\n",
    "#                      ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = \"Phrase\"\n",
    "# cross_validation.cross_val_score(pipeline, train[predictors], train[\"Sentiment\"], cv=3).mean()\n",
    "pipeline = pipeline.fit(sentences.CleanPhrase, sentences.Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation to check the initial score and fit to actually submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 ..., 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline.predict(test.CleanPhrase)\n",
    "print prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction gives around 60 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-43786a7920a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"PhraseId\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PhraseId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sentiment\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Use pandas to write the comma-separated output file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame( data={\"PhraseId\":test[\"PhraseId\"], \"Sentiment\":prediction} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(\"new.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "From the bag of words Kaggle competition we found a tutorial about Google's Word2Vec and decided to implement it on our dataset [Tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review):\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec requires lists of sentences so we had to group our data by sentenceId. We do not actually need a tokenizer to split the paragraphs into sentences as shown in the tutorial because our dataset is already split by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentencesWord2Vec = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for sentence in sentences[\"Phrase\"]:\n",
    "    sentencesWord2Vec.append(review_to_wordlist(sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gander', 'some', 'of', 'which', 'occasionally', 'amuses', 'but', 'none', 'of', 'which', 'amounts', 'to', 'much', 'of', 'a', 'story']\n",
      "['this', 'quiet', 'introspective', 'and', 'entertaining', 'independent', 'is', 'worth', 'seeking']\n"
     ]
    }
   ],
   "source": [
    "print sentencesWord2Vec[0]\n",
    "print sentencesWord2Vec[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We checked the first couple of sentences to confirm we had created the correct data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentencesWord2Vec, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'man'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('old', 0.9997593760490417),\n",
       " ('style', 0.999756395816803),\n",
       " ('predictable', 0.9997518062591553),\n",
       " ('action', 0.9997439384460449),\n",
       " ('feels', 0.9997289180755615),\n",
       " ('cinematic', 0.9997148513793945),\n",
       " ('often', 0.9997051954269409),\n",
       " ('american', 0.9997024536132812),\n",
       " ('family', 0.9996999502182007),\n",
       " ('premise', 0.9996915459632874)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pretty', 0.9996630549430847),\n",
       " ('half', 0.9996534585952759),\n",
       " ('especially', 0.999642014503479),\n",
       " ('done', 0.9996370673179626),\n",
       " ('simply', 0.9996294975280762),\n",
       " ('day', 0.9996140003204346),\n",
       " ('last', 0.9996137619018555),\n",
       " ('show', 0.9996005892753601),\n",
       " ('actually', 0.9995996952056885),\n",
       " ('give', 0.9995940923690796)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('still', 0.9997092485427856),\n",
       " ('going', 0.9996057152748108),\n",
       " ('how', 0.9996010065078735),\n",
       " ('because', 0.9995409250259399),\n",
       " ('mind', 0.9995290040969849),\n",
       " ('then', 0.9995182752609253),\n",
       " ('go', 0.9994921684265137),\n",
       " ('why', 0.9994859099388123),\n",
       " ('end', 0.9994835257530212),\n",
       " ('anyone', 0.9994621276855469)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our model took a very short time and as we can see the was not very accurate at all. doesnt_match function and most_similar don't seem to be working very well. Word2Vec works better as the size of its training data grows. We were only able to provide the model around 8544 sentences which is very small. Word2Vec is also supposed to work with a lot of unlabeld text so we had to disregard the Sentiment data we had. which goes out to say that we weren't really using the best model for our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
