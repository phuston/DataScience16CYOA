{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotten Tomatoes Model Iteration 1\n",
    "#### Patrick Huston and James Jang\n",
    "\n",
    "This notebook aims to make a first pass at producing a model capable of predicting sentiment on given phrases taken from movie reviews on Rotten Tomatoes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, imports! We'll be attempting to tackle this problem from a wide array of angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.tsv\", sep= '\\t')\n",
    "test = pd.read_csv(\"data/test.tsv\", sep= '\\t')\n",
    "# unlabeled = del train[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1\n",
    "\n",
    "As a very basic first pass, we'll try and create a model using the features we built in our data exploration notebook. While some of these features did seem to have positive correlations with the data, we don't have high hopes for the performance of this model - this mainly serves as a point of reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by dropping in the helper functions we created in the data exploration to facilitate the creation of our new features. These new features are:\n",
    "\n",
    "1. The number of words in a given phrase -- We noticed a strong correlation between the number of words and the standard deviation of the sentiment. While this wouldn't help us classify long phrases, it gives a pretty good indication that short phrases will most likely be of sentiment score 2. \n",
    "\n",
    "2. The length of the phrase in total -- This will be included for the same reason that the number of words is being used. Including both probably won't do much, but hey, let's try it anyway and see what happens.\n",
    "\n",
    "3. The average word length in a given phrase -- In our exploration, we came across some strange trends in the data relating to the average word length and its effect on the sentiment of a given phrase. We're not sure what predictive power this feature will end up having, but we'll give it a try nonetheless.\n",
    "\n",
    "4. Whether the phrase contains one or more of the most positvely correlated words in the corpus -- Pretty self-explanatory. This seems like it could help, but there are also negations and other patterns of language that could diminish this feature's predictive power.\n",
    "\n",
    "5. Whether the phrase contains one or more of the most negatively correlated words in the corpus -- same reasoning as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_phrase(phrase):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", phrase)\n",
    "    lower_case = letters_only.lower()\n",
    "    \n",
    "    words = lower_case.split()\n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return(\" \".join( meaningful_words))   \n",
    "\n",
    "def num_words(phrase):\n",
    "    return len(phrase.split())\n",
    "\n",
    "def length_phrase(phrase):\n",
    "    return len(phrase)\n",
    "\n",
    "def avg_word_length(phrase):\n",
    "    if(phrase != ''):\n",
    "        return sum(map(len, phrase.split()))/len(phrase.split())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "most_positive = ['remarkable', 'brilliant', 'terrific', 'excellent', 'finest', 'extraordinary', 'masterful', \n",
    "                 'hilarious', 'beautiful', 'wonderful', 'breathtaking', 'powerful', 'wonderfully', 'delightful', \n",
    "                 'masterfully', 'fantastic', 'dazzling', 'funniest', 'interference', 'refreshing']\n",
    "most_negative = ['worst', 'failure', 'lacks', 'waste', 'bore', 'depressing', 'lacking', 'stupid', 'disappointment', \n",
    "                 'unfunny', 'lame', 'devoid', 'trash', 'lousy', 'junk', 'poorly', 'mess', 'sleep', 'unappealing', 'fails']\n",
    "\n",
    "def contains_positive(phrase):\n",
    "    for word in phrase.split():\n",
    "        if word in most_positive:\n",
    "            return 1 \n",
    "    return 0\n",
    "        \n",
    "def contains_negative(phrase):\n",
    "    for word in phrase.split():\n",
    "        if word in most_negative:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even further convenience, we've wrapped the functionality of our feature creation functions into one helper function that does everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_transform(data):\n",
    "    data['CleanPhrase'] = data['Phrase'].apply(clean_phrase)\n",
    "    data['NumWords'] = data['CleanPhrase'].apply(num_words)\n",
    "    data['LengthPhrase'] = data['CleanPhrase'].apply(length_phrase)\n",
    "    data['AvgWordLength'] = data['CleanPhrase'].apply(avg_word_length)\n",
    "    data['ContainPositive'] = data['CleanPhrase'].apply(contains_positive)\n",
    "    data['ContainNegative'] = data['CleanPhrase'].apply(contains_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our transformation functions to the training and testing datasets now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apply_transform(train)\n",
    "apply_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned and features have been created, let's try running some models on the dataset. We'll try a couple of different options - first a logistic regression, and then perhaps a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression mean score: 0.525015974421\n",
      "Random forest mean score: 0.519191245532\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"ContainPositive\", \"ContainNegative\", \"NumWords\", \"LengthPhrase\", \"AvgWordLength\"]\n",
    "# predictors = [\"ContainPositive\", \"ContainNegative\"]\n",
    "logisticReg = LogisticRegression(random_state=1)\n",
    "randomForest = RandomForestClassifier(random_state=1, n_estimators=1000, min_samples_split=8, min_samples_leaf=4)\n",
    "mean_score_logistic = cross_validation.cross_val_score(logisticReg, train[predictors], train[\"Sentiment\"], cv=3).mean()\n",
    "mean_score_forest = cross_validation.cross_val_score(randomForest, train[predictors], train[\"Sentiment\"], cv=3).mean()\n",
    "\n",
    "print \"Logistic regression mean score: {}\".format(mean_score_logistic)\n",
    "print \"Random forest mean score: {}\".format(mean_score_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around the 50% mark isn't far from where we expected such a simple model would fall. Clearly our heroic efforts at initial attempts of creating numerical features from text data haven't gotten us very far. We're going to need to call in a bigger, badder, model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2: Enter bag of words/tfidf/count vectorizer! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by doing some research on how scikit-learn can deal with text data, and after some poking around, we stumbled up on a [Tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) that goes over the process of combining the powers of [tfidf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and a supervised learning model, like some of the models mentionted in the scikit-learn [Multiclass and Multilabel](http://scikit-learn.org/stable/modules/multiclass.html) docs. \n",
    "\n",
    "As suggested by the scikit-learn docs mentioned above, we started with a MultinomailNB naive Bayes model --\n",
    "\n",
    "```\n",
    "Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant.\n",
    "```\n",
    "\n",
    "Additionally, we implemented this all using the awesome scikit-learn Pipeline class that behaves like a compound classifier. \n",
    "\n",
    "Essentially what it does is this ~ vectorizer => transformer => classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipelineBayes = Pipeline([('vect', CountVectorizer()),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created another pipeline at the same time to try out a different training model - the [OneVsOneClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier) strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelineOneVOne = Pipeline([('vect', CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000)),\n",
    "                            ('tfidf', TfidfTransformer()),\n",
    "                            ('clf', OneVsOneClassifier(LinearSVC())),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our pipelines constructed, let's get some idea to see if all of our work has paid off - some simple scikit-learn cross validation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score for Bayes model: 0.553844611375\n",
      "Mean score for OnevOne Model: 0.592489915764\n"
     ]
    }
   ],
   "source": [
    "predictors = \"Phrase\"\n",
    "\n",
    "bayes_mean = cross_validation.cross_val_score(pipelineBayes, train[predictors], train[\"Sentiment\"], cv=3).mean()\n",
    "onevone_mean = cross_validation.cross_val_score(pipelineOneVOne, train[predictors], train[\"Sentiment\"], cv=3).mean()\n",
    "\n",
    "print \"Mean score for Bayes model: {}\".format(bayes_mean)\n",
    "print \"Mean score for OnevOne Model: {}\".format(onevone_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Nearly 60% is a marked improvement over previous models, and with some additional tuning, we were able to get up to ~62% on a Kaggle submission. Speaking of Kaggle submissions, let's create some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelineBayes = pipelineBayes.fit(train.Phrase, train.Sentiment)\n",
    "pipelineOneVOne = pipelineOneVOne.fit(train.Phrase, train.Sentiment)\n",
    "\n",
    "predictionBayes = pipelineBayes.predict(test.Phrase)\n",
    "predictionOnevOne = pipelineOneVOne.predict(test.Phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When submitted to Kaggle, these submissions receive scores of around 57% and 60%. We have more work to do, but this feels like a pretty good benchmark to start from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"PhraseId\":test[\"PhraseId\"], \"Sentiment\":predictionOnevOne} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(\"submission.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Future Iterations\n",
    "\n",
    "In future iterations, we aim to explore some of the following methods more.\n",
    "\n",
    "1. Training Word2Vec on a larger dataset\n",
    "2. Implement a porter-stemmer algorithm on the data\n",
    "3. Utilize the fact that the test data does include the SentenceId - we're currently analyzing only based on the raw text, and including\n",
    "4. See if we can figure out how to include negations \n",
    "5. Look around at existing APIs - like pattern - that could help us improve our submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
