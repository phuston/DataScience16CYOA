{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotten Tomatoes Sentiment Analysis \n",
    "#### Patrick Huston and James Jang\n",
    "\n",
    "This notebook aims to explore a revised model for the sentiment analysis Kaggle Rotten Tomatoes competition. Taking what we've learned from our exploration and first iteration model, we hope to improve our techniques and validation of modeling choices in the pursuit of a higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "1. Write a easily replicable testing/validation technique so we can easily verify new decisions/choices\n",
    "2. Write more analysis on why a technique is working better/worse\n",
    "3. Documentation as we go - write more markdown cells\n",
    "4. Specific cleaning exploration\n",
    "    - Unigram vs. bigram\n",
    "    - Negations\n",
    "5. Models\n",
    "    - Logistic regression\n",
    "    - SVM\n",
    "    - Tuning, tuning, tuning!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Techniques/Creating Features\n",
    "\n",
    "In natural language processing, the main 'feature' models use is the text itself - and there are several ways to extract numerical values from text. Additionally, there are cleaning steps and techniques that can be taken to improve the representation of the text inputted into the model.\n",
    "\n",
    "One of the first cleaning techniques that we tried was removing all of the punctuation and turning all of words into lower case. We performed this cleaning technique to normalize our dataset a bit. However sometimes capitalization and punctuation could affect the sentiment of the sentence so this cleaning technique might not always be the best.\n",
    "\n",
    "Another cleaning technique that we used was removing stopwords. Stopwords in english are words that are hold no meaning in the overall sentences. Words like the ,and, of etc are common stopwords that does not really contribute to the overall sentiment of the sentence.\n",
    "\n",
    "Next, we implemented some additional cleaning techniques used to further normalize the data - porter stemming and lemmatization. Porter stemming is the process of removing common morphological and inflexional endings from words in English. This is accomplished using simple algorithms that don't have any inherent knowledge of the English language, instead applying a set of rules to break down words and remove endings. Lemmatization, on the other hand, uses an input English dictionary to apply more intelligent breakdown of words based on part of speech. Unfortunately, lemmatization requires that every word be tagged with part of speech, which is an additional data processing step that, in the end, offered no real improvement in accuracy. For this reason, we decided to stick with the simpler algorithm, porter stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the dataset\n",
    "train = pd.read_csv(\"data/train.tsv\", sep= '\\t')\n",
    "test = pd.read_csv(\"data/test.tsv\", sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# negations = ['no', 'never', 'not']\n",
    "\n",
    "def clean_phrase_simple(phrase):\n",
    "    # Grab only words and lower them\n",
    "    clean_str = re.findall(r'\\w+', phrase, flags = re.UNICODE | re.LOCALE)\n",
    "    return ' '.join(clean_str).lower()\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_phrase_porter(phrase):\n",
    "    # \n",
    "    clean_str = re.findall(r'\\w+', phrase, flags = re.UNICODE | re.LOCALE)\n",
    "    stemmed = [porter_stemmer.stem(word) for word in clean_str]\n",
    "    return ' '.join(stemmed).lower()\n",
    "    \n",
    "# I tried something with negations here - didn't seem to offer any real improvement\n",
    "    \n",
    "#     for i, word in enumerate(meaningful_words):\n",
    "#         if word in negations or word.endswith('n\\'t'):\n",
    "#             try:\n",
    "#                 meaningful_words[i+1] = \"!\" + meaningful_words[i+1]\n",
    "#             except:\n",
    "#                 pass\n",
    "#             try:\n",
    "#                 meaningful_words[i-1] = \"!\" + meaningful_words[i-1]\n",
    "#             except:\n",
    "#                 pass      \n",
    "#     return(\" \".join( meaningful_words))   \n",
    "\n",
    "def clean_phrase_lemmatizer(phrase):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", phrase)\n",
    "    lower_case = letters_only.lower()\n",
    "    \n",
    "    words = lower_case.split()\n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    meaningful_words = [wordnet_lemmatizer.lemmatize(w) for w in words if not w in stops]\n",
    "    return(\" \".join( meaningful_words))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_transform(data):\n",
    "    data['CleanPhrase'] = data['Phrase'].apply(clean_phrase_porter)\n",
    "    data['CleanPhraseSimple'] = data['Phrase'].apply(clean_phrase_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apply_transform(train)\n",
    "apply_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Numerical Features from Text Data\n",
    "\n",
    "Now that we've explored some different methods of preprocessing the text, it's time to get into the machine learning. This will involve representing our text-based data numerically and then using this representation to fit a model to. \n",
    "\n",
    "One such way of representing text as numerical features is TFIDF - term-frequency inverse-document-frequency. Term frequency involves computing the total number of times each given token in a document appears. Inverse document frequency is an additional step that normalizes for the frequency of appearance of each token in the overall corpus. Inverse document frequency diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. \n",
    "\n",
    "To compute the TFIDF for our data, we use scikit-learn's TfidfVectorizer, a convenient tool that takes care of the details. In reality, however, TFIDF is relatively simple to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(train, test, column):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(train[column])\n",
    "    \n",
    "    X = vectorizer.transform(train[column])\n",
    "    X_test = vectorizer.transform(test[column])\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "\n",
    "X, X_test = vectorize(train, test, 'Phrase')\n",
    "\n",
    "X_cleaned, X_test_clean = vectorize(train, test, 'CleanPhraseSimple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Models\n",
    "\n",
    "To facilitate the process of testing a bunch of models, we've made the process easier by defining some convenience methods that iterate over a dictionary of models that we create. Through this process, we'll be able to compare each model's performance against the others, and make an informed decision.\n",
    "\n",
    "Here are all of the models we'll be trying:\n",
    "\n",
    "1. Logistic Regression\n",
    "    - We'll start with a simple multi-class logistic regression model. A common mistake in machine learning, especially in natural language processing, is to ignore the simpler models in lieu of the sexier new techniques - the likes of deep learning and neural networks. If nothing else, this will serve as a good benchmark for the rest of our models.\n",
    "\n",
    "2. Logistic Regression - Tuned\n",
    "    - Moving on from the logistic regression benchmark, let's also create a tuned vesion, which defines a list of penalty weights to test from. This will give us a better idea of how much tuning can help.\n",
    "    \n",
    "3. Random Forest Model\n",
    "    - Random forests can be great - it mostly depends on what the data looks like. In this case, we're a little hesitant about its ability to model the data as it seems to be more linear in many aspects than all over the place and random, which a set of decision trees could model more accurately.\n",
    "    \n",
    "4. Multinomial Naive Bayes\n",
    "    - In a lot of text analysis, it is somewhat common practice to start with a Naive Bayes model as a good baseline benchmark. Additionally, it is useful when there are limited resources in terms of CPU and Memory - it can be trained very quickly. \n",
    "    \n",
    "5. Support Vector Machine\n",
    "    - We also chose to test a linear implement of scikit-learns Support Vector Machine (SVM). SVM models are generally known to be good choices in cases of very high dimensionality, and the TFIDF vectors we have created fit the bill perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "logistic = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "\n",
    "# Tuned logistic regression\n",
    "logisticTune = LogisticRegressionCV(Cs=[math.e**v for v in range(-5,5)],\n",
    "                                    multi_class='multinomial',\n",
    "                                    solver='newton-cg')\n",
    "\n",
    "# Random forest model\n",
    "random = RandomForestClassifier()\n",
    "\n",
    "# Multinomail Naive Bayes\n",
    "multinomial = MultinomialNB()\n",
    "\n",
    "# Support Vector Machine Linear SVC\n",
    "SVM = svm.LinearSVC(penalty = 'l2', dual = False, tol = 1e-3)\n",
    "\n",
    "# Compile dictionary of models for later use\n",
    "models = {'Logistic': logistic, 'TunedLogistic': logisticTune, 'RandomForest': random, 'Multinomial' : multinomial, 'SVM': SVM}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the testing, let's define some convenience methods that we can use to break down the tasks involved in getting some validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross-validates model within trainnig set with a split of 'cv' - default value of 3\n",
    "def cross_validate(model, X, y, cv=3):\n",
    "    return cross_validation.cross_val_score(model, X, y, cv=cv).mean()\n",
    " \n",
    "# Performs train-test split on data, trains on train, tests on test, returns score\n",
    "def train_test_splitter(model, X, y, train_size=0.5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)\n",
    "\n",
    "# iterates over all different models and print out their results of train_test_splitter\n",
    "def test_models(models, X):\n",
    "    for modelName, model in models.iteritems():\n",
    "        print modelName\n",
    "        print train_test_splitter(model, X, train.Sentiment, train_size=0.5)\n",
    "\n",
    "# test one specific model with train_test_splitter\n",
    "def test_model(model, X):\n",
    "    return train_test_splitter(model, X, train.Sentiment, train_size=0.5)       \n",
    "\n",
    "# trains the model on the whole dataset, predicts on the test set and creates a submission file to kaggle\n",
    "def train_submit(model, X_train, y_train, X_test, filename = \"submission.csv\"):\n",
    "    print \"fitting\"\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    output = pd.DataFrame( data={\"PhraseId\":test[\"PhraseId\"], \"Sentiment\":prediction} )\n",
    "\n",
    "    # Use pandas to write the comma-separated output file\n",
    "    output.to_csv(filename, index=False, quoting=3 )\n",
    "    print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ### Testing Models\n",
    " \n",
    " Now that we have a good sampling of models defined and some good functions ready to do some intelligent testing, let's get right to it! All we have to do at this point is call our `test_models` function, and wait while the results come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Not Cleaned ------\n",
      "Multinomial\n",
      "0.574394463668\n",
      "RandomForest\n",
      "0.606100217865\n",
      "SVM\n",
      "0.62891195694\n",
      "Logistic\n",
      "0.623926694861\n",
      "------ Cleaned ------\n",
      "Multinomial\n",
      "0.574727668845\n",
      "RandomForest\n",
      "0.610085864411\n",
      "SVM\n",
      "0.629719338716\n",
      "Logistic\n",
      "0.624144559785\n"
     ]
    }
   ],
   "source": [
    "print \"------ No Preprocessing ------\"\n",
    "test_models(models, X)\n",
    "print \"-------- Preprocessed --------\"\n",
    "test_models(models, X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the Exploration\n",
    "\n",
    "Back in our data exploration, we noticed a strong positive correlation between the length of the phrase and the standard deviation of the sentiment. In other words, a high proportion of short phrases ended up with a neutral sentiment score of 2, while longer phrases were much more all over the place - scoring many more 0s, 1s, 3s, and 4s. For this reason, let's look into whether the addition of the number of words as a feature will add any accuracy to the model. Below, let's define an `add_word_length` function that will add an extra column to the TFIDF feature vector for the model to learn from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_word_length(X, data):\n",
    "    num_words_feature = np.asarray(map(lambda x: len(x.split()), data.Phrase))\n",
    "    num_words_feature = num_words_feature[:, np.newaxis]\n",
    "    return sparse.hstack((X, num_words_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put this function to use and do some validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_with_word_length = add_word_length(X, train)\n",
    "X_test = add_word_length(X_test, test)\n",
    "# test_models(models, X_with_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_model_improvment(model1, model2, X):\n",
    "    first = test_model(model1, X)\n",
    "    second = test_model(model2, X)\n",
    "    print \"model 1\", first\n",
    "    print \"model 2\", second\n",
    "    print \"difference in the score\", second - first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 0.625797770088\n",
      "model 2 0.637959759067\n",
      "difference in the score 0.0121619889786\n"
     ]
    }
   ],
   "source": [
    "compare_model_improvment(logistic, logisticTune, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda2/lib/python2.7/site-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/patrick/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:193: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    }
   ],
   "source": [
    "train_submit(logisticTune, X_with_word_length, train.Sentiment, X_test, filename = \"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('rotten_tomatoes_train.pickle')\n",
    "X = pickle.load(f)\n",
    "y = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
