{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.tsv\", sep= '\\t')\n",
    "test = pd.read_csv(\"data/test.tsv\", sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "negations = ['no', 'never', 'not']\n",
    "\n",
    "def clean_phrase_simple(phrase):\n",
    "    # Grab only words and lower them\n",
    "    clean_str = re.findall(r'\\w+', phrase, flags = re.UNICODE | re.LOCALE)\n",
    "    return ' '.join(clean_str).lower()\n",
    "\n",
    "def clean_phrase_porter(phrase):\n",
    "    clean_str = re.findall(r'\\w+', phrase, flags = re.UNICODE | re.LOCALE)\n",
    "    \n",
    "    stemmed = [porter_stemmer.stem(word) for word in clean_str]\n",
    "    \n",
    "    return ' '.join(stemmed).lower()\n",
    "    \n",
    "    return ' '.join(clean_str).lower()\n",
    "    \n",
    "    words = lower_case.split()\n",
    "#     stops = set(stopwords.words(\"english\")) \n",
    "    meaningful_words = [porter_stemmer.stem(w)]\n",
    "    \n",
    "    \n",
    "# I tried something with negations here - didn't seem to offer any real improvement\n",
    "    \n",
    "#     for i, word in enumerate(meaningful_words):\n",
    "#         if word in negations or word.endswith('n\\'t'):\n",
    "#             try:\n",
    "#                 meaningful_words[i+1] = \"!\" + meaningful_words[i+1]\n",
    "#             except:\n",
    "#                 pass\n",
    "#             try:\n",
    "#                 meaningful_words[i-1] = \"!\" + meaningful_words[i-1]\n",
    "#             except:\n",
    "#                 pass\n",
    "        \n",
    "        \n",
    "    return(\" \".join( meaningful_words))   \n",
    "\n",
    "def clean_phrase_lemmatizer(phrase):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", phrase)\n",
    "    lower_case = letters_only.lower()\n",
    "    \n",
    "    words = lower_case.split()\n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    meaningful_words = [wordnet_lemmatizer.lemmatize(w) for w in words if not w in stops]\n",
    "    return(\" \".join( meaningful_words))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_transform(data):\n",
    "    data['CleanPhrase'] = data['Phrase'].apply(clean_phrase_porter)\n",
    "    data['CleanPhraseSimple'] = data['Phrase'].apply(clean_phrase_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-a99fb767864a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mapply_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-37021c54b851>\u001b[0m in \u001b[0;36mapply_transform\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CleanPhrase'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Phrase'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_phrase_porter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CleanPhraseSimple'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Phrase'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_phrase_simple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/patrick/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2167\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2169\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:62578)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-528a797f1ffc>\u001b[0m in \u001b[0;36mclean_phrase_porter\u001b[1;34m(phrase)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlower_case\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#     stops = set(stopwords.words(\"english\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mmeaningful_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mporter_stemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "apply_transform(train)\n",
    "apply_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Bayes - apparently a good place to start, but haven't had huge success with it. Most places recommend that SVM/some onevone classifier would be better, so that's what we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipelineOneVOne = Pipeline([('vect', CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words='english', max_features = 5000)),\n",
    "                            ('tfidf', TfidfTransformer()),\n",
    "                            ('clf', OneVsOneClassifier(LinearSVC())),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipelineBayes = Pipeline([('vect', CountVectorizer()),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipelineSVM  = Pipeline([('vect', CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words='english', max_features = 5000)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', svm.LinearSVC(penalty = 'l2', dual = False, tol = 1e-3)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score for OnevOne Model: 0.592489915764  |  Predictor: CleanPhraseSimple\n",
      "Mean score for SVM Model: 0.586338448863  |  Predictor: CleanPhraseSimple\n",
      "Mean score for OnevOne Model: 0.588446596871  |  Predictor: CleanPhrase\n",
      "Mean score for SVM Model: 0.582858998162  |  Predictor: CleanPhrase\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"CleanPhraseSimple\",\"CleanPhrase\"]\n",
    "\n",
    "for predictor in predictors:\n",
    "    # bayes_mean = cross_validation.cross_val_score(pipelineBayes, train[predictors], train[\"Sentiment\"], cv=3).mean()\n",
    "    onevone_mean = cross_validation.cross_val_score(pipelineOneVOne, train[predictor], train[\"Sentiment\"], cv=3).mean()\n",
    "    svc_mean = cross_validation.cross_val_score(pipelineSVM, train[predictor], train[\"Sentiment\"], cv=3).mean()\n",
    "\n",
    "\n",
    "    # print \"Mean score for Bayes model: {}\".format(bayes_mean)\n",
    "    print \"Mean score for OnevOne Model: {}  |  Predictor: {}\".format(onevone_mean, predictor)\n",
    "    print \"Mean score for SVM Model: {}  |  Predictor: {}\".format(svc_mean, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipelineBayes = pipelineBayes.fit(train.Phrase, train.Sentiment)\n",
    "pipelineOneVOne = pipelineOneVOne.fit(train.Phrase, train.Sentiment)\n",
    "\n",
    "predictionBayes = pipelineBayes.predict(test.Phrase)\n",
    "predictionOnevOne = pipelineOneVOne.predict(test.Phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"PhraseId\":test[\"PhraseId\"], \"Sentiment\":predictionOnevOne} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(\"submission.csv\", index=False, quoting=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
