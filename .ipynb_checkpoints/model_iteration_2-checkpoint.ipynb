{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotten Tomatoes Sentiment Analysis \n",
    "#### Patrick Huston and James Jang\n",
    "\n",
    "This notebook aims to explore a revised model for the sentiment analysis Kaggle Rotten Tomatoes competition. Taking what we've learned from our exploration and first iteration model, we hope to improve our techniques and validation of modeling choices in the pursuit of a higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "1. Write a easily replicable testing/validation technique so we can easily verify new decisions/choices\n",
    "2. Write more analysis on why a technique is working better/worse\n",
    "3. Documentation as we go - write more markdown cells\n",
    "4. Specific cleaning exploration\n",
    "    - Unigram vs. bigram\n",
    "    - Negations\n",
    "5. Models\n",
    "    - Logistic regression\n",
    "    - SVM\n",
    "    - Tuning, tuning, tuning!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Techniques/Creating Features\n",
    "\n",
    "In natural language processing, the main 'feature' models use is the text itself - and there are several ways to extract numerical values from text. Additionally, there are cleaning steps and techniques that can be taken to improve the representation of the text inputted into the model.\n",
    "\n",
    "One of the first cleaning techniques that we tried was removing all of the punctuation and turning all of words into lower case. We performed this cleaning technique to normalize our dataset a bit. However sometimes capitalization and punctuation could affect the sentiment of the sentence so this cleaning technique might not always be the best.\n",
    "\n",
    "Another cleaning technique that we used was removing stopwords. Stopwords in english are words that are hold no meaning in the overall sentences. Words like the ,and, of etc are common stopwords that does not really contribute to the overall sentiment of the sentence.\n",
    "\n",
    "Next, we implemented some additional cleaning techniques used to further normalize the data - porter stemming and lemmatization. Porter stemming is the process of removing common morphological and inflexional endings from words in English. This is accomplished using simple algorithms that don't have any inherent knowledge of the English language, instead applying a set of rules to break down words and remove endings. Lemmatization, on the other hand, uses an input English dictionary to apply more intelligent breakdown of words based on part of speech. Unfortunately, lemmatization requires that every word be tagged with part of speech, which is an additional data processing step that, in the end, offered no real improvement in accuracy. For this reason, we decided to stick with the simpler algorithm, porter stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the dataset\n",
    "train = pd.read_csv(\"data/train.tsv\", sep= '\\t')\n",
    "test = pd.read_csv(\"data/test.tsv\", sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# negations = ['no', 'never', 'not']\n",
    "\n",
    "def clean_phrase_simple(phrase):\n",
    "    # Grab only words and lower them\n",
    "    clean_str = re.findall(r'\\w+', phrase, flags = re.UNICODE | re.LOCALE)\n",
    "    return ' '.join(clean_str).lower()\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_phrase_porter(phrase):\n",
    "    # \n",
    "    clean_str = re.findall(r'\\w+', phrase, flags = re.UNICODE | re.LOCALE)\n",
    "    stemmed = [porter_stemmer.stem(word) for word in clean_str]\n",
    "    return ' '.join(stemmed).lower()\n",
    "    \n",
    "# I tried something with negations here - didn't seem to offer any real improvement\n",
    "    \n",
    "#     for i, word in enumerate(meaningful_words):\n",
    "#         if word in negations or word.endswith('n\\'t'):\n",
    "#             try:\n",
    "#                 meaningful_words[i+1] = \"!\" + meaningful_words[i+1]\n",
    "#             except:\n",
    "#                 pass\n",
    "#             try:\n",
    "#                 meaningful_words[i-1] = \"!\" + meaningful_words[i-1]\n",
    "#             except:\n",
    "#                 pass      \n",
    "#     return(\" \".join( meaningful_words))   \n",
    "\n",
    "def clean_phrase_lemmatizer(phrase):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", phrase)\n",
    "    lower_case = letters_only.lower()\n",
    "    \n",
    "    words = lower_case.split()\n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    meaningful_words = [wordnet_lemmatizer.lemmatize(w) for w in words if not w in stops]\n",
    "    return(\" \".join( meaningful_words))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_transform(data):\n",
    "    data['CleanPhrase'] = data['Phrase'].apply(clean_phrase_porter)\n",
    "    data['CleanPhraseSimple'] = data['Phrase'].apply(clean_phrase_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apply_transform(train)\n",
    "apply_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_word_length(X, data):\n",
    "    num_words_feature = np.asarray(map(lambda x: len(x.split()), data.Phrase))\n",
    "    num_words_feature = num_words_feature[:, np.newaxis]\n",
    "    return sparse.hstack((X, num_words_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross-validates model within trainnig set with a split of 'cv' - default value of 3\n",
    "def cross_validate(model, X, y, cv=3):\n",
    "    return cross_validation.cross_val_score(model, X, y, cv=cv).mean()\n",
    " \n",
    "# Performs train-test split on data, trains on train, tests on test, returns score\n",
    "def train_test_splitter(model, X, y, train_size=0.5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)\n",
    "\n",
    "# trains the model on the whole dataset, predicts on the test set and creates a submission file to kaggle\n",
    "def train_submit(model, X_train, y_train, X_test, filename = \"submission.csv\"):\n",
    "    print \"fitting\"\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    output = pd.DataFrame( data={\"PhraseId\":test[\"PhraseId\"], \"Sentiment\":prediction} )\n",
    "\n",
    "    # Use pandas to write the comma-separated output file\n",
    "    output.to_csv(filename, index=False, quoting=3 )\n",
    "    print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Models\n",
    "\n",
    "Here we have a list of models. We'll describe what they are, why we chose them, and maybe some section about their strengths, weaknesses, and ability to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train.Phrase)\n",
    "\n",
    "vectorizer_clean = TfidfVectorizer()\n",
    "vectorizer_clean.fit(train.CleanPhrase)\n",
    "\n",
    "X = vectorizer.transform(train.Phrase)\n",
    "X_cleaned = vectorizer_clean.transform(train.CleanPhrase)\n",
    "\n",
    "X_test = vectorizer.transform(test.CleanPhrase)\n",
    "X_test = vectorizer_clean.transform(test.CleanPhrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "random = RandomForestClassifier()\n",
    "multinomial = MultinomialNB()\n",
    "SVM = svm.LinearSVC(penalty = 'l2', dual = False, tol = 1e-3)\n",
    "\n",
    "models = {'Logistic': logistic, 'RandomForest': random, 'Multinomial' : multinomial, 'SVM': SVM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterates over all different models and print out their results of train_test_splitter\n",
    "def test_models(models, X):\n",
    "    for modelName, model in models.iteritems():\n",
    "        print modelName\n",
    "        print train_test_splitter(model, X, train.Sentiment, train_size=0.5)\n",
    "\n",
    "# test one specific model with train_test_splitter\n",
    "def test_model(model, X):\n",
    "    return train_test_splitter(model, X, train.Sentiment, train_size=0.5)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Not Cleaned ------\n",
      "Multinomial\n",
      "0.574996796104\n",
      "RandomForest\n",
      "0.606625656799\n",
      "SVM\n",
      "0.631205946431\n",
      "Logistic\n",
      "0.625477380495\n",
      "------ Cleaned ------\n",
      "Multinomial\n",
      "0.572190183263\n",
      "RandomForest\n",
      "0.60553633218\n",
      "SVM\n",
      "0.625861848007\n",
      "Logistic\n",
      "0.624490580546\n"
     ]
    }
   ],
   "source": [
    "print \"------ Not Cleaned ------\"\n",
    "test_models(models, X)\n",
    "print \"------ Cleaned ------\"\n",
    "test_models(models, X_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_with_word_length = add_word_length(X, train)\n",
    "X_test = add_word_length(X_test, test)\n",
    "# test_models(models, X_with_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logisticTune = LogisticRegressionCV(Cs=[math.e**v for v in range(-5,5)],\n",
    "                             multi_class='multinomial',\n",
    "                             solver='newton-cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_model_improvment(model1, model2, X):\n",
    "    first = test_model(model1, X)\n",
    "    second = test_model(model2, X)\n",
    "    print \"model 1\", first\n",
    "    print \"model 2\", second\n",
    "    print \"difference in the score\", second - first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 0.625541458413\n",
      "model 2 0.637908496732\n",
      "difference in the score 0.0123670383186\n"
     ]
    }
   ],
   "source": [
    "compare_model_improvment(logistic, logisticTune, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_submit(logisticTune, X_with_word_length, train.Sentiment, X_test, filename = \"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('rotten_tomatoes_train.pickle')\n",
    "X = pickle.load(f)\n",
    "y = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
